{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings demo for the Data & AI Training day, November 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKTQRtGIfsJq"
   },
   "source": [
    "# Import libraries, mount google drive, cofnigure the api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pviIldgia2t_",
    "outputId": "8415b5b3-352a-4086-b5fc-52837b07f442"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# for loading the parameters:\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# for parsing API requests and responses:\n",
    "import json\n",
    "\n",
    "# for vector similarity metrics:\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "# for printing tables:\n",
    "from tabulate import tabulate\n",
    "# for HTML formatting in print():\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Openai version:0.28.1\n"
     ]
    }
   ],
   "source": [
    "#############################################################################################################################\n",
    "# Note: Because this script was written before openai 1.0.0, use the openai library 0.28.1.                                 #\n",
    "#       Alternatively, use openai migrate script to update the code to use hte latest OpenAI library                        #\n",
    "#        in which case refer to https://github.com/openai/openai-python/discussions/742 for the code upgrade instructions   #\n",
    "#############################################################################################################################\n",
    "%pip install openai==0.28.1 -q \n",
    "import openai\n",
    "print (f'Openai version:{openai.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVqQ4I0HlKI_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key loaded: sk-9e80...Oz50\n"
     ]
    }
   ],
   "source": [
    "# Load the environment variables from .env file in the notebook home folder and assigne the environment variables\n",
    "# in this case .env file contains the key generated at https://platform.openai.com/api-keys as a single row OPENAI_API_KEY=sk-...\n",
    "dotenv_path = r'C:\\Users\\sergey.lyubarskiy\\OneDrive - Accenture\\jupyter_notebooks'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Use os.environ to access environment variables\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "# Eye-ball if the key seem to have loaded correctly\n",
    "print(f'OpenAI API Key loaded: {OPENAI_API_KEY[:7]}...{OPENAI_API_KEY[-4:]}')  \n",
    "\n",
    "# Now initialise openai library with your key\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TpjbQeuf-2M"
   },
   "source": [
    "# Create embedding and calculate the distance between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yOzyavVlmu54",
    "outputId": "92d22f03-a1e8-4838-b5b2-210c7e82ce08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens usage: {\n",
      "  \"prompt_tokens\": 16,\n",
      "  \"total_tokens\": 16\n",
      "}\n",
      "Note a list of 3 objectcts returned, each containing a 1536-dimentional vector (as a list of float) \n",
      "Embedding 0: [-0.01904132030904293, 0.003049201099202037, -0.008756048046052456] ... (1536 values) ... [-0.03105313889682293, -0.00011234101839363575, -0.016920138150453568]\n",
      "Embedding 1: [-0.003592884400859475, 0.014437522739171982, 0.008472477085888386] ... (1536 values) ... [-0.013434549793601036, 0.0011926792794838548, -0.019096065312623978]\n",
      "Embedding 2: [0.012431960552930832, -0.00036493566585704684, 0.015292245894670486] ... (1536 values) ... [-0.0022573822643607855, 0.013759282417595387, -0.014868499711155891]\n"
     ]
    }
   ],
   "source": [
    "# Define input texts and get their embeddings\n",
    "\n",
    "input_texts = ['Cat is chasing a mouse','Kitten is catching rodents', 'Cats purr when happy']\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "# Call OpenAI to get embeddings for the input list of text strings\n",
    "\n",
    "# Docs: https://platform.openai.com/docs/api-reference/embeddings/create\n",
    "embeddings = openai.Embedding.create(input=input_texts, model=model_name)\n",
    "\n",
    "# Output object will have the keys 'object', 'data', 'model', 'usage', where the len(data) = len(input)\n",
    "\n",
    "# check the tokens used by the embeddings creation request\n",
    "print(f'tokens usage: {embeddings.usage}')\n",
    "\n",
    "# demonstrate that check the embeddings length is 1536\n",
    "print('Note a list of 3 objectcts returned, each containing a 1536-dimentional vector (as a list of float) ')\n",
    "for i in range(len(embeddings.data)):\n",
    "    # print first 3 and last 3 values of 1536 in each embedding\n",
    "    print(f'Embedding {i}: {embeddings.data[i].embedding[:3]} ... ({len(embeddings.data[i].embedding)} values) ... {embeddings.data[i].embedding[-3:]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GnFSwNWdZkB",
    "outputId": "93123881-30b1-48b4-d03b-8b4c70e3b00c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "The cosine similarity measures the cosine of the angle between two vectors, yielding a value between -1 and 1, where <B> 1 indicates identical vectors</B>. \n",
       "<BR>However <B>scipy</B>.spatial.distance.cosine function <B>outputs = 1 - cosine_similarity</B>.\n",
       "<BR>Thus, with <B>scipy</B> the cosine distance ranges from 0 to 2, where <B>0 indicates identical</B> vectors. \n",
       "<BR>This means, for identical vectors, the scipy.spatial.distance.cosine function will return 0, indicating no distance (or perfect similarity)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------+-------------------+----------------------+\n",
      "| Text 1                     | Text 2                     |   Cosine Distance |   Euclidean Distance |\n",
      "+============================+============================+===================+======================+\n",
      "| Cat is chasing a mouse     | Cat is chasing a mouse     |            0      |               0      |\n",
      "+----------------------------+----------------------------+-------------------+----------------------+\n",
      "| Cat is chasing a mouse     | Kitten is catching rodents |            0.0754 |               0.3884 |\n",
      "+----------------------------+----------------------------+-------------------+----------------------+\n",
      "| Cat is chasing a mouse     | Cats purr when happy       |            0.1518 |               0.551  |\n",
      "+----------------------------+----------------------------+-------------------+----------------------+\n",
      "| Kitten is catching rodents | Kitten is catching rodents |            0      |               0      |\n",
      "+----------------------------+----------------------------+-------------------+----------------------+\n",
      "| Kitten is catching rodents | Cats purr when happy       |            0.1679 |               0.5795 |\n",
      "+----------------------------+----------------------------+-------------------+----------------------+\n",
      "| Cats purr when happy       | Cats purr when happy       |            0      |               0      |\n",
      "+----------------------------+----------------------------+-------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "# For illustration print cosine and euclidean distances between embeddings.\n",
    "# note that distane to itself is minimum for identical sentence\n",
    "# note that either metric finds the closest sentences are:\n",
    "#    'Cat is chasing a mouse' and 'Kitten is catching rodents' \n",
    "\n",
    "# Calculate distances and store in a list\n",
    "table_data = []\n",
    "for i in range(len(embeddings.data)):\n",
    "    for j in range(i, len(embeddings.data)):\n",
    "        cosine_distance = scipy.spatial.distance.cosine(embeddings.data[i].embedding, embeddings.data[j].embedding)\n",
    "        euclidean_distance = scipy.spatial.distance.euclidean(embeddings.data[i].embedding, embeddings.data[j].embedding)\n",
    "        table_data.append([input_texts[i], input_texts[j], f\"{cosine_distance:.4f}\", f\"{euclidean_distance:.4f}\"])\n",
    "\n",
    "\n",
    "# pring, using HTML tags <B> for bold and <BR> for new line\n",
    "display(HTML('''\n",
    "The cosine similarity measures the cosine of the angle between two vectors, yielding a value between -1 and 1, where <B> 1 indicates identical vectors</B>. \n",
    "<BR>However <B>scipy</B>.spatial.distance.cosine function <B>outputs = 1 - cosine_similarity</B>.\n",
    "<BR>Thus, with <B>scipy</B> the cosine distance ranges from 0 to 2, where <B>0 indicates identical</B> vectors. \n",
    "<BR>This means, for identical vectors, the scipy.spatial.distance.cosine function will return 0, indicating no distance (or perfect similarity).'''\n",
    "))\n",
    "# Print as a table with headers\n",
    "headers = [\"Text 1\", \"Text 2\", \"Cosine Distance\", \"Euclidean Distance\"]\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4781Gzifn9q"
   },
   "source": [
    "# Create and search a vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings, initialise in-memory vector db and load the sentences,  initialise the index and load embeddings into the inedex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Sometimes faiss fails, but impirically found that installing mkl before faiss \n",
    "%pip install mkl -q\n",
    "%pip install faiss-cpu -q\n",
    "# import mkl # importing mkl is not required, just installing it before faiss may help to avoid errors\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "aA4fYb7YrseV"
   },
   "outputs": [],
   "source": [
    "# Now let's load the embeddings into a vector databse and search for the \"neighbors\" of the embedding included in the  search query\n",
    "# Generate embeddings (again to for illustrative purposes, so that the whole loading is self-contained)\n",
    "input_texts = ['Cat is chasing a mouse','Kitten is catching rodents', 'Cats purr when happy']\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "# Get embeddings for a list of text strings\n",
    "embeddings = openai.Embedding.create(input=input_texts, model=model_name)\n",
    "\n",
    "# create a numpy array of embeddings\n",
    "embeddings_list = []\n",
    "\n",
    "for emb in embeddings.data:\n",
    "  embeddings_list.append(emb.embedding)\n",
    "embeddings_np = np.array(embeddings_list)\n",
    "\n",
    "\n",
    "# Create a FAISS index for cosine similarity\n",
    "cossim_idx = faiss.IndexFlatIP(1536)\n",
    "cossim_idx.add (embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7_nMiOSpF99",
    "outputId": "9a9e7f6a-8eae-4a39-bd38-c60c5b219417"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<P>The <B> cosine </B> similarity measures the cosine of the angle between two vectors, yielding a value between -1 and 1, where <B> 1 indicates identical vectors</B>. \n",
       "<BR> <B>However </B>, FAISS's index.search function, when using a cosine similarity metric, actually returns the similarity score, not the distance. \n",
       "<BR> In many implementations, including FAISS, the similarity score is often scaled or shifted so that higher values indicate more similarity. \n",
       "<BR> Specifically, FAISS computes the inner product (not exactly the cosine similarity) when configured for maximum inner product search (MIPS). \n",
       "<BR> Threfore, with <B>FAISS perfect similarity = 1 or higher </B> (depending on the scaling)\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------------+----------------------------+-------------------+\n",
      "| Query Text             |   Database Index | Database Text              |   Cosine Distance |\n",
      "+========================+==================+============================+===================+\n",
      "| Cat is chasing a mouse |                0 | Cat is chasing a mouse     |            1      |\n",
      "+------------------------+------------------+----------------------------+-------------------+\n",
      "| Cat is chasing a mouse |                1 | Kitten is catching rodents |            0.9246 |\n",
      "+------------------------+------------------+----------------------------+-------------------+\n",
      "| Cat is chasing a mouse |                2 | Cats purr when happy       |            0.8482 |\n",
      "+------------------------+------------------+----------------------------+-------------------+\n",
      "+----------------------+------------------+----------------------------+-------------------+\n",
      "| Query Text           |   Database Index | Database Text              |   Cosine Distance |\n",
      "+======================+==================+============================+===================+\n",
      "| Dogs bark when angry |                2 | Cat is chasing a mouse     |            0.8495 |\n",
      "+----------------------+------------------+----------------------------+-------------------+\n",
      "| Dogs bark when angry |                0 | Kitten is catching rodents |            0.8025 |\n",
      "+----------------------+------------------+----------------------------+-------------------+\n",
      "| Dogs bark when angry |                1 | Cats purr when happy       |            0.7886 |\n",
      "+----------------------+------------------+----------------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "headers = [\"Query Text\", \"Database Index\", \"Database Text\", \"Cosine Distance\"]\n",
    "\n",
    "# create a list of queries\n",
    "query_texts = ['Cat is chasing a mouse', 'Dogs bark when angry']\n",
    "\n",
    "def get_embedding(input_text, model_name):\n",
    "    \"\"\"Get embedding for a single text using a specified model.\"\"\"\n",
    "    return openai.Embedding.create(input=input_text, model=model_name).data[0].embedding\n",
    "\n",
    "def search_in_index(index, query_embedding):\n",
    "    \"\"\"Search a query embedding in the FAISS index.\"\"\"\n",
    "    query_emb_arr = np.array(query_embedding).reshape(1, -1)\n",
    "    return index.search(query_emb_arr, 3)\n",
    "\n",
    "# Iteratively search database for each query and print the results\n",
    "display(HTML('''\n",
    "<P>The <B> cosine </B> similarity measures the cosine of the angle between two vectors, yielding a value between -1 and 1, where <B> 1 indicates identical vectors</B>. \n",
    "<BR> <B>However </B>, FAISS's index.search function, when using a cosine similarity metric, actually returns the similarity score, not the distance. \n",
    "<BR> In many implementations, including FAISS, the similarity score is often scaled or shifted so that higher values indicate more similarity. \n",
    "<BR> Specifically, FAISS computes the inner product (not exactly the cosine similarity) when configured for maximum inner product search (MIPS). \n",
    "<BR> Threfore, with <B>FAISS perfect similarity = 1 or higher </B> (depending on the scaling)\n",
    "'''\n",
    "))\n",
    "for query_text in query_texts:\n",
    "    # EMBED the query\n",
    "    query_embedding = get_embedding(query_text, model_name) # get the query's embedding for OpenAI\n",
    "\n",
    "    # SEARCH database for the query string, returning the matches and the corresponding distance \n",
    "    similarities, indices = search_in_index(cossim_idx, query_embedding) #search FAISS index for query's embedding\n",
    "\n",
    "    # PRINT search results as a table\n",
    "    table_data = []\n",
    "    for i in range(len(indices[0])):\n",
    "        table_data.append([query_text, f\"{indices[0][i]:.4f}\", input_texts[i], f\"{similarities[0][i]:.4f}\"])\n",
    "    print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4ogsFKvpWL8"
   },
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
